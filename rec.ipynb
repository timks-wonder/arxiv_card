{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4600 entries, 0 to 4599\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          4600 non-null   int64 \n",
      " 1   title       4600 non-null   object\n",
      " 2   authors     4600 non-null   object\n",
      " 3   categories  4600 non-null   object\n",
      " 4   published   4600 non-null   object\n",
      " 5   summary     4600 non-null   object\n",
      " 6   url         4600 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 251.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# 若无法解析导入“sentence_transformers”，需要安装该库\n",
    "# 可以使用以下命令进行安装\n",
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.preprocessing import MinMaxScale\n",
    "\n",
    "# 加载预训练模型（中文推荐'paraphrase-multilingual-MiniLM-L12-v2'）\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv('arxiv_data/arxiv_cv_papers.csv')\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n",
    "# # 保存向量\n",
    "# np.save('title_embeddings.npy', title_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    }
   ],
   "source": [
    "# 假设df是你的DataFrame对象，获取第一列（文章名字列）\n",
    "article_names = df.iloc[:, 1]\n",
    "article_names = article_names.tolist()\n",
    "print(len(article_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 生成标题向量\n",
    "title_embeddings = model.encode(article_names)  # 每个标题得到384维向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4600, 384)\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(title_embeddings.shape)\n",
    "title_embedding_mean = title_embeddings.mean(axis=0)\n",
    "print(title_embedding_mean.shape)\n",
    "\n",
    "np.save('user_data/ori_recal_embedding.npy', title_embedding_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "K = 1000  # 假设这里的k为5，你可以根据实际情况修改\n",
    "# 计算每个title embedding与title embedding mean的余弦相似度\n",
    "similarities = cosine_similarity(title_embeddings, title_embedding_mean.reshape(1, -1)).flatten()\n",
    "\n",
    "# 获取相似度最大的k个index\n",
    "top_K_indices = np.argsort(similarities)[-K:][::-1]\n",
    "# 获取df中这1000个topk indices的样本\n",
    "top_K_samples = df.iloc[top_K_indices]\n",
    "\n",
    "# for i in top_K_indices:\n",
    "#     print(article_published[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 1155 to 1197\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          1000 non-null   int64 \n",
      " 1   title       1000 non-null   object\n",
      " 2   authors     1000 non-null   object\n",
      " 3   categories  1000 non-null   object\n",
      " 4   published   1000 non-null   object\n",
      " 5   summary     1000 non-null   object\n",
      " 6   url         1000 non-null   object\n",
      " 7   embeddings  1000 non-null   object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 70.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "article_summary = top_K_samples.iloc[:, 5]\n",
    "article_summary = article_summary.tolist()\n",
    "\n",
    "summary_embeddings = model.encode(article_summary)  # 每个摘要得到384维向量\n",
    "# 将 summary_embeddings 转换为 DataFrame\n",
    "summary_embeddings_df = pd.DataFrame(summary_embeddings)\n",
    "\n",
    "# 在第 8 列插入 summary_embeddings_df\n",
    "top_K_samples.insert(7, 'embeddings', summary_embeddings_df.values.tolist())\n",
    "\n",
    "top_K_samples.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "summary_embeddings_mean = summary_embeddings.mean(axis=0)\n",
    "# 保存summary_embeddings_mean为npy文件\n",
    "np.save('user_data/ori_rank_embedding.npy', summary_embeddings_mean)\n",
    "\n",
    "sim_summary = cosine_similarity(summary_embeddings, summary_embeddings_mean.reshape(1, -1)).flatten()\n",
    "\n",
    "k = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重排"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 96, 50, 82, 634, 556, 647, 317, 4, 169, 749, 144, 178, 25, 562, 262, 542, 774, 175, 367, 415, 649, 198, 239, 424, 66, 406, 40, 398, 148, 494, 216, 941, 442, 692, 123, 28, 62, 0, 697, 798, 340, 846, 151, 880, 325, 308, 218, 276, 544, 255, 375, 336, 52, 970, 183, 5, 72, 97, 523, 916, 328, 610, 492, 474, 356, 240, 725, 1, 17, 19, 125, 67, 503, 21, 84, 290, 414, 452, 476, 745, 20, 532, 225, 867, 727, 948, 117, 345, 286, 912, 329, 294, 108, 29, 275, 94, 870, 219, 802]\n"
     ]
    }
   ],
   "source": [
    "# viewed = [410, 192, 615]\n",
    "viewed = []\n",
    "# 获取相似度最大的k个index\n",
    "top_k_indices = np.argsort(sim_summary)[-k:][::-1]\n",
    "# 可以使用布尔索引来替代列表推导式，以实现更优雅的矩阵操作。\n",
    "# 首先创建一个布尔数组，标记viewed中的元素为False\n",
    "mask = np.isin(np.arange(len(sim_summary)), viewed, invert=True)\n",
    "# 然后结合argsort和布尔索引来筛选出不在viewed中的top_k_indices\n",
    "top_k_indices = np.argsort(sim_summary[mask])[-k:][::-1]\n",
    "# 由于使用了布尔索引，需要映射回原始索引\n",
    "top_k_indices = np.arange(len(sim_summary))[mask][top_k_indices]\n",
    "top_k_indices = [idx for idx in top_k_indices if idx not in viewed]\n",
    "\n",
    "\n",
    "# 获取df中这1000个topk indices的样本\n",
    "top_k_samples = top_K_samples.iloc[top_k_indices]\n",
    "\n",
    "print(top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              title  \\\n",
      "2616  2617  RealGeneral: Unifying Visual Generation via Te...   \n",
      "2352  2353  TACO: Taming Diffusion for in-the-wild Video A...   \n",
      "1729  1730  TULIP: Towards Unified Language-Image Pretraining   \n",
      "3361  3362  ARMOR v0.1: Empowering Autoregressive Multimod...   \n",
      "1144  1145                         Equivariant Image Modeling   \n",
      "...    ...                                                ...   \n",
      "380    381  JointTuner: Appearance-Motion Adaptive Joint T...   \n",
      "1052  1053  Improved Alignment of Modalities in Large Visi...   \n",
      "1367  1368  good4cir: Generating Detailed Synthetic Captio...   \n",
      "2788  2789  ForAug: Recombining Foregrounds and Background...   \n",
      "3550  3551  Unified Reward Model for Multimodal Understand...   \n",
      "\n",
      "                                                authors  \\\n",
      "2616  Yijing Lin, Mengqi Huang, Shuhan Zhuang, Zhend...   \n",
      "2352  Ruijie Lu, Yixin Chen, Yu Liu, Jiaxiang Tang, ...   \n",
      "1729  Zineng Tang, Long Lian, Seun Eisape, XuDong Wa...   \n",
      "3361  Jianwen Sun, Yukang Feng, Chuanhao Li, Fanrui ...   \n",
      "1144  Ruixiao Dong, Mengde Xu, Zigang Geng, Li Li, H...   \n",
      "...                                                 ...   \n",
      "380    Fangda Chen, Shanshan Zhao, Chuanfu Xu, Long Lan   \n",
      "1052  Kartik Jangra, Aman Kumar Singh, Yashwani Mann...   \n",
      "1367  Pranavi Kolouju, Eric Xing, Robert Pless, Nath...   \n",
      "2788  Tobias Christian Nauen, Brian Moser, Federico ...   \n",
      "3550  Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, Ji...   \n",
      "\n",
      "                                            categories   published  \\\n",
      "2616                                      cs.CV, cs.AI  2025-03-13   \n",
      "2352                                             cs.CV  2025-03-15   \n",
      "1729                        cs.CV, cs.AI, cs.CL, cs.LG  2025-03-19   \n",
      "3361                                      cs.CV, cs.AI  2025-03-09   \n",
      "1144                                             cs.CV  2025-03-24   \n",
      "...                                                ...         ...   \n",
      "380                                              cs.CV  2025-03-31   \n",
      "1052                                      cs.CV, cs.LG  2025-03-25   \n",
      "1367                                      cs.CV, cs.AI  2025-03-22   \n",
      "2788  cs.CV, cs.AI, cs.LG, 68T45, I.2.10; I.2.6; I.4.6  2025-03-12   \n",
      "3550                                             cs.CV  2025-03-07   \n",
      "\n",
      "                                                summary  \\\n",
      "2616  Unifying diverse image generation tasks within...   \n",
      "2352  Humans can infer complete shapes and appearanc...   \n",
      "1729  Despite the recent success of image-text contr...   \n",
      "3361  Unified models (UniMs) for multimodal understa...   \n",
      "1144  Current generative models, such as autoregress...   \n",
      "...                                                 ...   \n",
      "380   Recent text-to-video advancements have enabled...   \n",
      "1052  Recent advancements in vision-language models ...   \n",
      "1367  Composed image retrieval (CIR) enables users t...   \n",
      "2788  Transformers, particularly Vision Transformers...   \n",
      "3550  Recent advances in human preference alignment ...   \n",
      "\n",
      "                                    url  \\\n",
      "2616  http://arxiv.org/abs/2503.10406v1   \n",
      "2352  http://arxiv.org/abs/2503.12049v1   \n",
      "1729  http://arxiv.org/abs/2503.15485v1   \n",
      "3361  http://arxiv.org/abs/2503.06542v1   \n",
      "1144  http://arxiv.org/abs/2503.18948v1   \n",
      "...                                 ...   \n",
      "380   http://arxiv.org/abs/2503.23951v1   \n",
      "1052  http://arxiv.org/abs/2503.19508v1   \n",
      "1367  http://arxiv.org/abs/2503.17871v1   \n",
      "2788  http://arxiv.org/abs/2503.09399v1   \n",
      "3550  http://arxiv.org/abs/2503.05236v1   \n",
      "\n",
      "                                             embeddings  \n",
      "2616  [-0.05033300071954727, -0.10798574984073639, 0...  \n",
      "2352  [0.011743038892745972, -0.11219225078821182, 0...  \n",
      "1729  [-0.041819628328084946, -0.0909130722284317, 0...  \n",
      "3361  [-0.03198173642158508, -0.15321817994117737, 0...  \n",
      "1144  [-0.014911633916199207, -0.10221464186906815, ...  \n",
      "...                                                 ...  \n",
      "380   [-0.03792077302932739, -0.17649996280670166, 0...  \n",
      "1052  [-0.03782389312982559, -0.05569641664624214, -...  \n",
      "1367  [-0.028854241594672203, -0.026958277449011803,...  \n",
      "2788  [-0.05350338667631149, -0.09956572204828262, 0...  \n",
      "3550  [-0.04721811041235924, -0.12950585782527924, -...  \n",
      "\n",
      "[100 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(top_k_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设top_k_samples是一个列表或者DataFrame\n",
    "# 如果top_k_samples是列表，将其转换为DataFrame\n",
    "if not isinstance(top_k_samples, pd.DataFrame):\n",
    "    top_k_samples = pd.DataFrame(top_k_samples)\n",
    "\n",
    "# 保存为CSV文件\n",
    "top_k_samples.to_csv('arxiv_data/arxiv_samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
